# YOLOv5 on Triton Inference Server or Deepstream with TensorRT

This repository shows how to deploy YOLOv5 as an optimized [TensorRT] engine to [Triton Inference Server].

This project is based on (https://github.com/wang-xinyu/tensorrtx)  
This project is based on (https://github.com/NVIDIA/triton-inference-server)  
This project is based on (https://github.com/marcoslucianops/DeepStream-Yolo)  
This project is based on (https://github.com/ultralytics/yolov5)  

### Requirements

* [Ubuntu] 20.04
* [python]	>=3.6.9<3.7
* [docker-ce]	>19.03.5	
* [nvidia-container-toolkit] >1.3.0-1	
* [nvidia-container-runtime]	3.4.0-1	
* [nvidia-docker2]	2.5.0-1	
* [nvidia-driver]	> 515	
* [python-pip]	> 21.06	

### Getting started

* [YOLOv5 Deepstream usage](docs/Deepstream.md)
* [YOLOv5 Triton usage](docs/Triton.md)


